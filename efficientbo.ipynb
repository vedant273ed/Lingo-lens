{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad523ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.5\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(mp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4222038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B0 initialized\n",
      "Device: cpu\n",
      "Number of classes: 30\n",
      "Training samples: 2380\n",
      "Validation samples: 688\n",
      "Starting training...\n",
      " Starting training for 15 epochs...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]:   0%|          | 0/149 [00:00<?, ?it/s]e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/15 [Train]:  11%|â–ˆ         | 16/149 [01:05<09:01,  4.07s/it, Loss=3.2595, Acc=0.0139]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 294\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mtop_class\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    270\u001b[39m classifier = EfficientNetISLClassifier(data_dir)\n\u001b[32m    272\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_efficientnet_isl2_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    276\u001b[39m classifier.evaluate_model(\u001b[33m\"\u001b[39m\u001b[33mbest_efficientnet_isl_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mEfficientNetISLClassifier.train\u001b[39m\u001b[34m(self, num_epochs, learning_rate, save_path)\u001b[39m\n\u001b[32m     98\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(inputs)\n\u001b[32m     99\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m optimizer.step()\n\u001b[32m    103\u001b[39m train_running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "class EfficientNetISLClassifier:\n",
    "    def __init__(self, data_dir, num_classes=None, device=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Transforms\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.setup_datasets()\n",
    "        self.model = self.create_model()\n",
    "        self.best_acc = 0.0\n",
    "        self.class_names = self.train_dataset.classes\n",
    "        \n",
    "        print(f\"EfficientNet-B0 initialized\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Number of classes: {len(self.class_names)}\")\n",
    "        print(f\"Training samples: {len(self.train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(self.val_dataset)}\")\n",
    "    \n",
    "    def setup_datasets(self):\n",
    "        self.train_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"train\"), transform=self.train_transforms)\n",
    "        \n",
    "        self.val_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"valid\"), transform=self.val_transforms)\n",
    "        \n",
    "        test_path = os.path.join(self.data_dir, \"test\")\n",
    "        if os.path.exists(test_path):\n",
    "            self.test_dataset = datasets.ImageFolder(test_path, transform=self.val_transforms)\n",
    "        else:\n",
    "            self.test_dataset = None\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        if self.test_dataset:\n",
    "            self.test_loader = DataLoader(self.test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "        num_classes = len(self.train_dataset.classes)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def train(self, num_epochs=50, learning_rate=0.001, save_path=\"best_efficientnet_model1.pth\"):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        print(f\" Starting training for {num_epochs} epochs...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.model.train()\n",
    "            train_running_loss = 0.0\n",
    "            train_running_corrects = 0\n",
    "\n",
    "            train_pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                train_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                train_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{train_running_corrects.double() / len(self.train_dataset):.4f}'\n",
    "                })\n",
    "\n",
    "            epoch_loss = train_running_loss / len(self.train_dataset)\n",
    "            epoch_acc = train_running_corrects.double() / len(self.train_dataset)\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accuracies.append(epoch_acc.item())\n",
    "\n",
    "            self.model.eval()\n",
    "            val_running_corrects = 0\n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "                for inputs, labels in val_pbar:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_running_corrects += torch.sum(preds == labels.data)\n",
    "                    val_pbar.set_postfix({\n",
    "                        'Acc': f'{val_running_corrects.double() / len(self.val_dataset):.4f}'\n",
    "                    })\n",
    "\n",
    "            val_acc = val_running_corrects.double() / len(self.val_dataset)\n",
    "            val_accuracies.append(val_acc.item())\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Time: {time.time() - start_time:.2f}s\")\n",
    "            print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_acc': self.best_acc,\n",
    "                    'class_names': self.class_names\n",
    "                }, save_path)\n",
    "                print(f\"New best model saved! Accuracy: {val_acc:.4f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        print(f\"Training completed! Best validation accuracy: {self.best_acc:.4f}\")\n",
    "        self.plot_training_history(train_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_acc': self.best_acc.item()\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self, train_losses, train_accuracies, val_accuracies):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.plot(train_losses, label='Training Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Training history plot saved as 'training_history.png'\")\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.best_acc = checkpoint['best_acc']\n",
    "        self.class_names = checkpoint['class_names']\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        print(f\"Best accuracy: {self.best_acc:.4f}\")\n",
    "\n",
    "    def test_on_image(self, image_path, model_path=None):\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.val_transforms(image)\n",
    "        input_batch = input_tensor.unsqueeze(0).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_batch)\n",
    "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probabilities, 5)\n",
    "        top_probs = top_probs.cpu().numpy()\n",
    "        top_indices = top_indices.cpu().numpy()\n",
    "\n",
    "        print(f\"\\nTesting image: {image_path}\")\n",
    "        print(\"=\"*50)\n",
    "        for i in range(len(top_probs)):\n",
    "            class_name = self.class_names[top_indices[i]]\n",
    "            confidence = top_probs[i] * 100\n",
    "            print(f\"#{i+1}: {class_name} - {confidence:.2f}%\")\n",
    "\n",
    "        return {\n",
    "            'predictions': [(self.class_names[idx], prob) for idx, prob in zip(top_indices, top_probs)],\n",
    "            'top_class': self.class_names[top_indices[0]],\n",
    "            'confidence': top_probs[0]\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model_path=None, use_test_set=False):\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "        if use_test_set and self.test_dataset:\n",
    "            eval_loader = self.test_loader\n",
    "            eval_dataset = self.test_dataset\n",
    "            set_name = \"Test\"\n",
    "        else:\n",
    "            eval_loader = self.val_loader\n",
    "            eval_dataset = self.val_dataset\n",
    "            set_name = \"Validation\"\n",
    "\n",
    "        print(f\"Evaluating model on {set_name} set...\")\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        print(f\"\\n {set_name} Set Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\n Classification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=self.class_names))\n",
    "\n",
    "        self.plot_confusion_matrix(all_labels, all_preds)\n",
    "        return {'accuracy': accuracy, 'predictions': all_preds, 'labels': all_labels}\n",
    "\n",
    "    def plot_confusion_matrix(self, labels, predictions):\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Confusion matrix saved as 'confusion_matrix1.png'\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_dir = r\"E:\\New folder\\isl_inverted\"\n",
    "    classifier = EfficientNetISLClassifier(data_dir)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    classifier.train(num_epochs=15, learning_rate=0.001, save_path=\"best_efficientnet_isl2_model.pth\")\n",
    "\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    classifier.evaluate_model(\"best_efficientnet_isl_model.pth\")\n",
    "\n",
    "    print(\"\\nTesting on sample images...\")\n",
    "    val_samples = []\n",
    "    for class_idx, class_name in enumerate(classifier.class_names):\n",
    "        class_path = os.path.join(data_dir, \"valid\", class_name)\n",
    "        if os.path.exists(class_path):\n",
    "            images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if images:\n",
    "                val_samples.append(os.path.join(class_path, random.choice(images)))\n",
    "\n",
    "    for i, image_path in enumerate(val_samples[:3]):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        result = classifier.test_on_image(image_path, \"best_efficientnet_isl_model.pth\")\n",
    "        print(f\"Predicted: {result['top_class']} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa4a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "âœ… Webcam started. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# âœ… Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# âœ… Load EfficientNet-B0 model\n",
    "model = models.efficientnet_b0(pretrained=False)\n",
    "num_classes = 30\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load weights\n",
    "checkpoint = torch.load(\"best_efficientnet_isl_model.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… Class names\n",
    "class_names = checkpoint[\"class_names\"]\n",
    "\n",
    "# âœ… Define transform (same as during validation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# âœ… MediaPipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# âœ… Predict function\n",
    "def predict_hand(frame):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        h, w, _ = frame.shape\n",
    "        x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                x_min = min(x, x_min)\n",
    "                y_min = min(y, y_min)\n",
    "                x_max = max(x, x_max)\n",
    "                y_max = max(y, y_max)\n",
    "\n",
    "        # Add padding\n",
    "        pad = 30\n",
    "        x_min = max(x_min - pad, 0)\n",
    "        y_min = max(y_min - pad, 0)\n",
    "        x_max = min(x_max + pad, w)\n",
    "        y_max = min(y_max + pad, h)\n",
    "\n",
    "        cropped_hand = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        if cropped_hand.size == 0:\n",
    "            return None, 0.0\n",
    "\n",
    "        # Transform and predict\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(cropped_hand, cv2.COLOR_BGR2RGB))\n",
    "        input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "            pred_index = torch.argmax(probs).item()\n",
    "            confidence = probs[pred_index].item()\n",
    "\n",
    "        return class_names[pred_index], confidence\n",
    "    else:\n",
    "        return None, 0.0\n",
    "\n",
    "# âœ… Webcam\n",
    "cap = cv2.VideoCapture(r\"E:\\New folder\\WhatsApp Video 2025-07-05 at 00.13.38_242d5edf.mp4\")\n",
    "print(\"âœ… Webcam started. Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pred_class, confidence = predict_hand(frame)\n",
    "\n",
    "    if pred_class:\n",
    "        text = f\"{pred_class} ({confidence*100:.1f}%)\"\n",
    "        cv2.putText(frame, text, (10, 35),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        cv2.putText(frame, \"No hand detected\", (10, 35),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"ISL Real-Time\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526381f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86111f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam started. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA3\\envs\\cv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "model = models.efficientnet_b0(pretrained=False)\n",
    "num_classes = 30\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load weights\n",
    "checkpoint = torch.load(\"optuna.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "class_names = checkpoint[\"class_names\"]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def predict_hand(frame):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        h, w, _ = frame.shape\n",
    "        x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                x_min = min(x, x_min)\n",
    "                y_min = min(y, y_min)\n",
    "                x_max = max(x, x_max)\n",
    "                y_max = max(y, y_max)\n",
    "\n",
    "        pad = 30\n",
    "        x_min = max(x_min - pad, 0)\n",
    "        y_min = max(y_min - pad, 0)\n",
    "        x_max = min(x_max + pad, w)\n",
    "        y_max = min(y_max + pad, h)\n",
    "\n",
    "        cropped_hand = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        if cropped_hand.size == 0:\n",
    "            return None, 0.0\n",
    "\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(cropped_hand, cv2.COLOR_BGR2RGB))\n",
    "        input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "            pred_index = torch.argmax(probs).item()\n",
    "            confidence = probs[pred_index].item()\n",
    "\n",
    "        return class_names[pred_index], confidence\n",
    "    else:\n",
    "        return None, 0.0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Webcam started. Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pred_class, confidence = predict_hand(frame)\n",
    "\n",
    "    if pred_class:\n",
    "        text = f\"{pred_class} ({confidence*100:.1f}%)\"\n",
    "        cv2.putText(frame, text, (10, 35),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        cv2.putText(frame, \"No hand detected\", (10, 35),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"ISL Real-Time\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm # ðŸ†• For progress bars\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP (Paths and Transforms)\n",
    "# ==============================================================================\n",
    "data_dir = r\"E:\\New folder\\isl_dataset-cropped\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Just normalization for validation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA LOADING\n",
    "# ==============================================================================\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"valid\"), transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Found {num_classes} classes: {class_names}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MODEL SETUP\n",
    "# ==============================================================================\n",
    "# ðŸ†• Use the modern 'weights' parameter instead of 'pretrained=True'\n",
    "model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
    "\n",
    "# ðŸ†• Correctly get the number of input features for the classifier\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "# ðŸ†• Correctly replace the final layer\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ðŸ†• Add a learning rate scheduler to decrease LR over time\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TRAINING LOOP\n",
    "# ==============================================================================\n",
    "num_epochs = 20\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # ðŸ†• Use tqdm for a progress bar\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # ðŸ†• Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # ðŸ†• Calculate validation loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_corrects.double() / len(val_dataset)\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "    # ðŸ†• Save the best model based on validation accuracy\n",
    "    if val_epoch_acc > best_acc:\n",
    "        best_acc = val_epoch_acc\n",
    "        # ðŸ†• Save a checkpoint dictionary for easier use later\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'class_names': class_names\n",
    "        }\n",
    "        torch.save(checkpoint, 'best_efficientnet_isl_model4.pth')\n",
    "        print(f\"ðŸŽ‰ New best model saved with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import optuna # ðŸ†• Import Optuna\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP (Paths and Transforms)\n",
    "# ==============================================================================\n",
    "data_dir = r\"E:\\New folder\\isl_dataset-cropped\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Just normalization for validation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets once\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"valid\"), transform=val_transforms)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"This function trains a model with a set of hyperparameters and returns its validation accuracy.\"\"\"\n",
    "    \n",
    "    # --- 2.1 Define the Hyperparameter Search Space ---\n",
    "    \n",
    "    # -- Optimizer and Learning Rate --\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # -- Data Augmentation Strength --\n",
    "    rotation_degrees = trial.suggest_int(\"rotation_degrees\", 0, 25)\n",
    "    color_jitter_brightness = trial.suggest_float(\"color_jitter_brightness\", 0.1, 0.5)\n",
    "    color_jitter_contrast = trial.suggest_float(\"color_jitter_contrast\", 0.1, 0.5)\n",
    "    \n",
    "    # -- Model Hyperparameters --\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # --- 2.2 Define Transforms and DataLoaders FOR THIS TRIAL ---\n",
    "    # Create the transforms using the parameters suggested for this trial\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=rotation_degrees),\n",
    "        transforms.ColorJitter(brightness=color_jitter_brightness, contrast=color_jitter_contrast),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms remain fixed\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create new dataset objects with the trial-specific transforms\n",
    "    train_dataset_trial = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "    \n",
    "    # We can reuse the global validation dataset since its transform doesn't change\n",
    "    train_loader = DataLoader(train_dataset_trial, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # --- 2.3 Model Setup ---\n",
    "    model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # --- 2.4 Optimizer and Loss (with optimizer-specific params) ---\n",
    "    if optimizer_name == \"SGD\":\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.85, 0.99)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else: # RMSprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --- 2.5 Training Loop ---\n",
    "    num_epochs = 15 # Keep epochs lower for faster tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # --- 2.6 Validation ---\n",
    "    model.eval()\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_acc = val_corrects.double() / len(val_dataset)\n",
    "\n",
    "    # --- 2.7 Return the metric you want to optimize ---\n",
    "    return val_acc\n",
    "\n",
    "# 3. RUN THE OPTIMIZATION STUDY\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a study. 'direction=\"maximize\"' means we want to find the highest validation accuracy.\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50) # You can change n_trials to 20, 50, 100 etc.\n",
    "    \n",
    "    print(\"\\n\\n--- OPTIMIZATION FINISHED ---\")\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    \n",
    "    print(\"\\n--- BEST TRIAL ---\")\n",
    "    trial = study.best_trial\n",
    "    print(\"Value (Best Validation Accuracy): \", trial.value)\n",
    "    \n",
    "    print(\"\\n--- BEST HYPERPARAMETERS ---\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import optuna # ðŸ†• Import Optuna\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP (Paths and Transforms)\n",
    "# ==============================================================================\n",
    "data_dir = r\"E:\\New folder\\isl_inverted\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Just normalization for validation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets once\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"valid\"), transform=val_transforms)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"This function trains a model with a set of hyperparameters and returns its validation accuracy.\"\"\"\n",
    "    \n",
    "    # --- 2.1 Define the Hyperparameter Search Space ---\n",
    "    \n",
    "    # -- Optimizer and Learning Rate --\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # -- Data Augmentation Strength --\n",
    "    rotation_degrees = trial.suggest_int(\"rotation_degrees\", 0, 25)\n",
    "    color_jitter_brightness = trial.suggest_float(\"color_jitter_brightness\", 0.1, 0.5)\n",
    "    color_jitter_contrast = trial.suggest_float(\"color_jitter_contrast\", 0.1, 0.5)\n",
    "    \n",
    "    # -- Model Hyperparameters --\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # --- 2.2 Define Transforms and DataLoaders FOR THIS TRIAL ---\n",
    "    # Create the transforms using the parameters suggested for this trial\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=rotation_degrees),\n",
    "        transforms.ColorJitter(brightness=color_jitter_brightness, contrast=color_jitter_contrast),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms remain fixed\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create new dataset objects with the trial-specific transforms\n",
    "    train_dataset_trial = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "    \n",
    "    # We can reuse the global validation dataset since its transform doesn't change\n",
    "    train_loader = DataLoader(train_dataset_trial, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # --- 2.3 Model Setup ---\n",
    "    model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # --- 2.4 Optimizer and Loss (with optimizer-specific params) ---\n",
    "    if optimizer_name == \"SGD\":\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.85, 0.99)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else: # RMSprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --- 2.5 Training Loop ---\n",
    "    num_epochs = 15 # Keep epochs lower for faster tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # --- 2.6 Validation ---\n",
    "    model.eval()\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_acc = val_corrects.double() / len(val_dataset)\n",
    "\n",
    "    # --- 2.7 Return the metric you want to optimize ---\n",
    "    return val_acc\n",
    "\n",
    "# 3. RUN THE OPTIMIZATION STUDY\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a study. 'direction=\"maximize\"' means we want to find the highest validation accuracy.\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50) # You can change n_trials to 20, 50, 100 etc.\n",
    "    \n",
    "    print(\"\\n\\n--- OPTIMIZATION FINISHED ---\")\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    \n",
    "    print(\"\\n--- BEST TRIAL ---\")\n",
    "    trial = study.best_trial\n",
    "    print(\"Value (Best Validation Accuracy): \", trial.value)\n",
    "    \n",
    "    print(\"\\n--- BEST HYPERPARAMETERS ---\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54d1ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch [1/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 1/25 | Train Loss: 3.0713 | Train Acc: 0.1092 | Val Loss: 3.0159 | Val Acc: 0.1453\n",
      "\n",
      "Epoch [2/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 2/25 | Train Loss: 2.3447 | Train Acc: 0.2992 | Val Loss: 2.1039 | Val Acc: 0.3677\n",
      "\n",
      "Epoch [3/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 3/25 | Train Loss: 1.4413 | Train Acc: 0.5794 | Val Loss: 1.6618 | Val Acc: 0.5727\n",
      "\n",
      "Epoch [4/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 4/25 | Train Loss: 0.9295 | Train Acc: 0.7261 | Val Loss: 0.9259 | Val Acc: 0.7282\n",
      "\n",
      "Epoch [5/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 5/25 | Train Loss: 0.4781 | Train Acc: 0.8689 | Val Loss: 0.7201 | Val Acc: 0.7936\n",
      "\n",
      "Epoch [6/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 6/25 | Train Loss: 0.2374 | Train Acc: 0.9307 | Val Loss: 0.7647 | Val Acc: 0.8387\n",
      "\n",
      "Epoch [7/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 7/25 | Train Loss: 0.2202 | Train Acc: 0.9361 | Val Loss: 0.6517 | Val Acc: 0.8430\n",
      "\n",
      "Epoch [8/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 | Train Loss: 0.1188 | Train Acc: 0.9676 | Val Loss: 0.7267 | Val Acc: 0.8372\n",
      "\n",
      "Epoch [9/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 9/25 | Train Loss: 0.1653 | Train Acc: 0.9483 | Val Loss: 0.4194 | Val Acc: 0.8924\n",
      "\n",
      "Epoch [10/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 | Train Loss: 0.1862 | Train Acc: 0.9437 | Val Loss: 0.4861 | Val Acc: 0.8866\n",
      "\n",
      "Epoch [11/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 | Train Loss: 0.0607 | Train Acc: 0.9836 | Val Loss: 0.5193 | Val Acc: 0.8735\n",
      "\n",
      "Epoch [12/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 12/25 | Train Loss: 0.0452 | Train Acc: 0.9870 | Val Loss: 0.4130 | Val Acc: 0.9142\n",
      "\n",
      "Epoch [13/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 | Train Loss: 0.0288 | Train Acc: 0.9920 | Val Loss: 0.4514 | Val Acc: 0.9026\n",
      "\n",
      "Epoch [14/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 | Train Loss: 0.0696 | Train Acc: 0.9782 | Val Loss: 0.7266 | Val Acc: 0.8343\n",
      "\n",
      "Epoch [15/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 | Train Loss: 0.1135 | Train Acc: 0.9676 | Val Loss: 1.0622 | Val Acc: 0.7892\n",
      "\n",
      "Epoch [16/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 | Train Loss: 0.1409 | Train Acc: 0.9571 | Val Loss: 0.5508 | Val Acc: 0.8387\n",
      "\n",
      "Epoch [17/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 | Train Loss: 0.0659 | Train Acc: 0.9773 | Val Loss: 0.4364 | Val Acc: 0.8953\n",
      "\n",
      "Epoch [18/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 | Train Loss: 0.0254 | Train Acc: 0.9933 | Val Loss: 0.4220 | Val Acc: 0.9113\n",
      "\n",
      "Epoch [19/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 | Train Loss: 0.0105 | Train Acc: 0.9966 | Val Loss: 0.3381 | Val Acc: 0.9142\n",
      "\n",
      "Epoch [20/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 | Train Loss: 0.0274 | Train Acc: 0.9933 | Val Loss: 0.4269 | Val Acc: 0.8924\n",
      "\n",
      "Epoch [21/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 | Train Loss: 0.0758 | Train Acc: 0.9744 | Val Loss: 0.9180 | Val Acc: 0.8023\n",
      "\n",
      "Epoch [22/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 | Train Loss: 0.1074 | Train Acc: 0.9651 | Val Loss: 0.5948 | Val Acc: 0.8910\n",
      "\n",
      "Epoch [23/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 | Train Loss: 0.0390 | Train Acc: 0.9903 | Val Loss: 0.4016 | Val Acc: 0.9084\n",
      "\n",
      "Epoch [24/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model saved\n",
      "Epoch 24/25 | Train Loss: 0.0063 | Train Acc: 0.9992 | Val Loss: 0.2847 | Val Acc: 0.9259\n",
      "\n",
      "Epoch [25/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 | Train Loss: 0.0108 | Train Acc: 0.9971 | Val Loss: 0.6025 | Val Acc: 0.8939\n",
      "\n",
      "Training complete! Best Val Accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# âœ… Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# âœ… Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# âœ… Dataset paths\n",
    "train_path = r\"E:\\New folder\\isl_inverted\\train\"\n",
    "valid_path = r\"E:\\New folder\\isl_inverted\\valid\"\n",
    "\n",
    "# âœ… Datasets and loaders\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(valid_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "from torchvision.models import efficientnet_b0\n",
    "model = EfficientNetB0(num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# âœ… Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# âœ… Training parameters\n",
    "num_epochs = 25\n",
    "best_val_acc = 0.0\n",
    "save_path = \"self.pth\"\n",
    "\n",
    "# âœ… Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Train\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training\", leave=False)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f\"{loss.item():.4f}\",\n",
    "            'Acc': f\"{(correct/total):.4f}\"\n",
    "        })\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"new model saved\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best Val Accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcd19b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Required packages:\n",
      "pip install torch torchvision matplotlib seaborn scikit-learn tqdm pillow\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA3\\envs\\cv_stable\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\ANACONDA3\\envs\\cv_stable\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EfficientNet-B3 initialized\n",
      "ðŸ“± Device: cuda\n",
      "ðŸŽ¯ Number of classes: 30\n",
      "ðŸ“Š Training samples: 2380\n",
      "ðŸ“Š Validation samples: 688\n",
      "ðŸš€ Starting training...\n",
      "ðŸš€ Starting training for 15 epochs...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:40<00:00,  3.50s/it, Loss=1.0761, Acc=0.5584]\n",
      "Epoch 1/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:28<00:00,  1.48it/s, Acc=0.8924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Time: 549.83s\n",
      "Train Loss: 1.6851 | Train Acc: 0.5584 | Val Acc: 0.8924\n",
      "âœ… New best model saved! Accuracy: 0.8924\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [07:51<00:00,  3.17s/it, Loss=1.2551, Acc=0.7958]\n",
      "Epoch 2/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:24<00:00,  1.75it/s, Acc=0.9549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Time: 496.36s\n",
      "Train Loss: 0.7394 | Train Acc: 0.7958 | Val Acc: 0.9549\n",
      "âœ… New best model saved! Accuracy: 0.9549\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:20<00:00,  3.36s/it, Loss=0.2649, Acc=0.8571]\n",
      "Epoch 3/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:30<00:00,  1.43it/s, Acc=0.9797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Time: 530.88s\n",
      "Train Loss: 0.5006 | Train Acc: 0.8571 | Val Acc: 0.9797\n",
      "âœ… New best model saved! Accuracy: 0.9797\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:07<00:00,  3.27s/it, Loss=0.2402, Acc=0.8702]\n",
      "Epoch 4/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:26<00:00,  1.63it/s, Acc=0.9622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Time: 513.82s\n",
      "Train Loss: 0.4728 | Train Acc: 0.8702 | Val Acc: 0.9622\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:04<00:00,  3.25s/it, Loss=0.2669, Acc=0.8811]\n",
      "Epoch 5/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:28<00:00,  1.49it/s, Acc=0.9433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Time: 513.18s\n",
      "Train Loss: 0.4113 | Train Acc: 0.8811 | Val Acc: 0.9433\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:17<00:00,  3.34s/it, Loss=0.0290, Acc=0.8979]\n",
      "Epoch 6/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:24<00:00,  1.76it/s, Acc=0.9680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Time: 521.93s\n",
      "Train Loss: 0.3694 | Train Acc: 0.8979 | Val Acc: 0.9680\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:15<00:00,  3.32s/it, Loss=0.1473, Acc=0.8971]\n",
      "Epoch 7/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:24<00:00,  1.75it/s, Acc=0.9767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Time: 519.74s\n",
      "Train Loss: 0.3936 | Train Acc: 0.8971 | Val Acc: 0.9767\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:11<00:00,  3.30s/it, Loss=0.0158, Acc=0.9210]\n",
      "Epoch 8/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:26<00:00,  1.64it/s, Acc=0.9826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Time: 518.15s\n",
      "Train Loss: 0.2844 | Train Acc: 0.9210 | Val Acc: 0.9826\n",
      "âœ… New best model saved! Accuracy: 0.9826\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:04<00:00,  3.25s/it, Loss=0.0224, Acc=0.9328]\n",
      "Epoch 9/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:25<00:00,  1.68it/s, Acc=0.9797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Time: 509.98s\n",
      "Train Loss: 0.2341 | Train Acc: 0.9328 | Val Acc: 0.9797\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:03<00:00,  3.25s/it, Loss=0.0035, Acc=0.9424]\n",
      "Epoch 10/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:26<00:00,  1.63it/s, Acc=0.9767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Time: 510.38s\n",
      "Train Loss: 0.1982 | Train Acc: 0.9424 | Val Acc: 0.9767\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [07:58<00:00,  3.21s/it, Loss=0.0098, Acc=0.9412]\n",
      "Epoch 11/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:25<00:00,  1.71it/s, Acc=0.9767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Time: 503.90s\n",
      "Train Loss: 0.1988 | Train Acc: 0.9412 | Val Acc: 0.9767\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [07:57<00:00,  3.21s/it, Loss=0.1184, Acc=0.9454]\n",
      "Epoch 12/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:25<00:00,  1.69it/s, Acc=0.9811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Time: 503.14s\n",
      "Train Loss: 0.1854 | Train Acc: 0.9454 | Val Acc: 0.9811\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:02<00:00,  3.24s/it, Loss=0.3897, Acc=0.9563]\n",
      "Epoch 13/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:24<00:00,  1.76it/s, Acc=0.9724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Time: 506.66s\n",
      "Train Loss: 0.1441 | Train Acc: 0.9563 | Val Acc: 0.9724\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:02<00:00,  3.24s/it, Loss=0.1288, Acc=0.9567]\n",
      "Epoch 14/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:25<00:00,  1.66it/s, Acc=0.9826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Time: 508.81s\n",
      "Train Loss: 0.1598 | Train Acc: 0.9567 | Val Acc: 0.9826\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [08:15<00:00,  3.32s/it, Loss=0.4961, Acc=0.9559]\n",
      "Epoch 15/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:27<00:00,  1.57it/s, Acc=0.9811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Time: 522.51s\n",
      "Train Loss: 0.1389 | Train Acc: 0.9559 | Val Acc: 0.9811\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¯ Training completed! Best validation accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Training history plot saved as 'training_history.png'\n",
      "\n",
      "ðŸ” Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedan\\AppData\\Local\\Temp\\ipykernel_27548\\878998462.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'b3_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 298\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpip install torch torchvision matplotlib seaborn scikit-learn tqdm pillow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    297\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 277\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    274\u001b[39m classifier.train(num_epochs=\u001b[32m15\u001b[39m, learning_rate=\u001b[32m0.001\u001b[39m, save_path=\u001b[33m\"\u001b[39m\u001b[33mbest_efficientnet_b3_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ” Evaluating model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb3_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ–¼ï¸ Testing on sample images...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m val_samples = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mEfficientNetB3ISLClassifier.evaluate_model\u001b[39m\u001b[34m(self, model_path, use_test_set)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path=\u001b[38;5;28;01mNone\u001b[39;00m, use_test_set=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_path:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_test_set \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.test_dataset:\n\u001b[32m    225\u001b[39m         eval_loader = \u001b[38;5;28mself\u001b[39m.test_loader\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mEfficientNetB3ISLClassifier.load_model\u001b[39m\u001b[34m(self, model_path)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path):\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_acc = checkpoint[\u001b[33m'\u001b[39m\u001b[33mbest_acc\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv_stable\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv_stable\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ANACONDA3\\envs\\cv_stable\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'b3_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "class EfficientNetB3ISLClassifier:\n",
    "    def __init__(self, data_dir, num_classes=None, device=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Transforms (adjusted for EfficientNet-B3 size)\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(300),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize(320),\n",
    "            transforms.CenterCrop(300),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Setup data\n",
    "        self.setup_datasets()\n",
    "        self.model = self.create_model()\n",
    "        self.best_acc = 0.0\n",
    "        self.class_names = self.train_dataset.classes\n",
    "        \n",
    "        print(f\"âœ… EfficientNet-B3 initialized\")\n",
    "        print(f\"ðŸ“± Device: {self.device}\")\n",
    "        print(f\"ðŸŽ¯ Number of classes: {len(self.class_names)}\")\n",
    "        print(f\"ðŸ“Š Training samples: {len(self.train_dataset)}\")\n",
    "        print(f\"ðŸ“Š Validation samples: {len(self.val_dataset)}\")\n",
    "    \n",
    "    def setup_datasets(self):\n",
    "        self.train_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"train\"), transform=self.train_transforms)\n",
    "        \n",
    "        self.val_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, \"valid\"), transform=self.val_transforms)\n",
    "        \n",
    "        test_path = os.path.join(self.data_dir, \"test\")\n",
    "        if os.path.exists(test_path):\n",
    "            self.test_dataset = datasets.ImageFolder(test_path, transform=self.val_transforms)\n",
    "        else:\n",
    "            self.test_dataset = None\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        if self.test_dataset:\n",
    "            self.test_loader = DataLoader(self.test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "        num_classes = len(self.train_dataset.classes)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def train(self, num_epochs=15, learning_rate=0.001, save_path=\"best_efficientnet_b3_model.pth\"):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        print(f\"ðŸš€ Starting training for {num_epochs} epochs...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.model.train()\n",
    "            train_running_loss = 0.0\n",
    "            train_running_corrects = 0\n",
    "\n",
    "            train_pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                train_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                train_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{train_running_corrects.double() / len(self.train_dataset):.4f}'\n",
    "                })\n",
    "\n",
    "            epoch_loss = train_running_loss / len(self.train_dataset)\n",
    "            epoch_acc = train_running_corrects.double() / len(self.train_dataset)\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accuracies.append(epoch_acc.item())\n",
    "\n",
    "            self.model.eval()\n",
    "            val_running_corrects = 0\n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "                for inputs, labels in val_pbar:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_running_corrects += torch.sum(preds == labels.data)\n",
    "                    val_pbar.set_postfix({\n",
    "                        'Acc': f'{val_running_corrects.double() / len(self.val_dataset):.4f}'\n",
    "                    })\n",
    "\n",
    "            val_acc = val_running_corrects.double() / len(self.val_dataset)\n",
    "            val_accuracies.append(val_acc.item())\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Time: {time.time() - start_time:.2f}s\")\n",
    "            print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_acc': self.best_acc,\n",
    "                    'class_names': self.class_names\n",
    "                }, save_path)\n",
    "                print(f\"âœ… New best model saved! Accuracy: {val_acc:.4f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        print(f\"ðŸŽ¯ Training completed! Best validation accuracy: {self.best_acc:.4f}\")\n",
    "        self.plot_training_history(train_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_acc': self.best_acc.item()\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self, train_losses, train_accuracies, val_accuracies):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.plot(train_losses, label='Training Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"ðŸ“Š Training history plot saved as 'training_history.png'\")\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.best_acc = checkpoint['best_acc']\n",
    "        self.class_names = checkpoint['class_names']\n",
    "        print(f\"âœ… Model loaded from {model_path}\")\n",
    "        print(f\"ðŸ“Š Best accuracy: {self.best_acc:.4f}\")\n",
    "\n",
    "    def test_on_image(self, image_path, model_path=None):\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.val_transforms(image)\n",
    "        input_batch = input_tensor.unsqueeze(0).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_batch)\n",
    "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probabilities, 5)\n",
    "        top_probs = top_probs.cpu().numpy()\n",
    "        top_indices = top_indices.cpu().numpy()\n",
    "\n",
    "        print(f\"\\nðŸ” Testing image: {image_path}\")\n",
    "        print(\"=\"*50)\n",
    "        for i in range(len(top_probs)):\n",
    "            class_name = self.class_names[top_indices[i]]\n",
    "            confidence = top_probs[i] * 100\n",
    "            print(f\"#{i+1}: {class_name} - {confidence:.2f}%\")\n",
    "\n",
    "        return {\n",
    "            'predictions': [(self.class_names[idx], prob) for idx, prob in zip(top_indices, top_probs)],\n",
    "            'top_class': self.class_names[top_indices[0]],\n",
    "            'confidence': top_probs[0]\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model_path=None, use_test_set=False):\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "        if use_test_set and self.test_dataset:\n",
    "            eval_loader = self.test_loader\n",
    "            eval_dataset = self.test_dataset\n",
    "            set_name = \"Test\"\n",
    "        else:\n",
    "            eval_loader = self.val_loader\n",
    "            eval_dataset = self.val_dataset\n",
    "            set_name = \"Validation\"\n",
    "\n",
    "        print(f\"ðŸ” Evaluating model on {set_name} set...\")\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        print(f\"\\nðŸ“Š {set_name} Set Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nðŸ“‹ Classification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=self.class_names))\n",
    "\n",
    "        self.plot_confusion_matrix(all_labels, all_preds)\n",
    "        return {'accuracy': accuracy, 'predictions': all_preds, 'labels': all_labels}\n",
    "\n",
    "    def plot_confusion_matrix(self, labels, predictions):\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"ðŸ“Š Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_dir = r\"E:\\New folder\\isl_inverted\"\n",
    "    classifier = EfficientNetB3ISLClassifier(data_dir)\n",
    "\n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "    classifier.train(num_epochs=15, learning_rate=0.001, save_path=\"best_efficientnet_b3_model.pth\")\n",
    "\n",
    "    print(\"\\nðŸ” Evaluating model...\")\n",
    "    classifier.evaluate_model(\"b3_model.pth\")\n",
    "\n",
    "    print(\"\\nðŸ–¼ï¸ Testing on sample images...\")\n",
    "    val_samples = []\n",
    "    for class_idx, class_name in enumerate(classifier.class_names):\n",
    "        class_path = os.path.join(data_dir, \"valid\", class_name)\n",
    "        if os.path.exists(class_path):\n",
    "            images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if images:\n",
    "                val_samples.append(os.path.join(class_path, random.choice(images)))\n",
    "\n",
    "    for i, image_path in enumerate(val_samples[:3]):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        result = classifier.test_on_image(image_path, \"best_efficientnet_b3_model.pth\")\n",
    "        print(f\"Predicted: {result['top_class']} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ“¦ Required packages:\")\n",
    "    print(\"pip install torch torchvision matplotlib seaborn scikit-learn tqdm pillow\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd96ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcec08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfa6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
